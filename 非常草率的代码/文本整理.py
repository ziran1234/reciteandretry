import nltk
from nltk.tokenize import word_tokenize
from collections import Counter

nltk.download('punkt')  # ç¡®ä¿ä¸‹è½½åˆ†è¯å™¨

def clean_text(text):
    """ æ¸…ç†ç™¾åº¦è¯­éŸ³è¯†åˆ«æ–‡æœ¬ï¼Œæ£€æµ‹é‡å¤å’Œä¸æµç•…éƒ¨åˆ† """
    words = word_tokenize(text)  # åˆ†è¯
    cleaned_words = []
    repetition_count = Counter()

    for i, word in enumerate(words):
        if i > 0 and word == words[i - 1]:  # å¤„ç†è¿ç»­é‡å¤
            repetition_count[word] += 1
            if repetition_count[word] > 2:  # è¶…è¿‡2æ¬¡æ ‡è®°ä¸ºâ€œä¸ç†Ÿç»ƒâ€
                continue
        else:
            repetition_count[word] = 1

        cleaned_words.append(word)

    cleaned_text = " ".join(cleaned_words)
    return cleaned_text, repetition_count

# ç¤ºä¾‹æ–‡æœ¬ï¼ˆç™¾åº¦è¯­éŸ³è¯†åˆ«è¾“å‡ºï¼‰
baidu_text = "åºŠå‰ æ˜æœˆå…‰ æ˜æœˆå…‰ ç–‘æ˜¯åœ°ä¸Šéœœ éœœ ä¸¾å¤´æœ›æ˜æœˆ ä½å¤´ ä½å¤´ æ€æ•…ä¹¡"

cleaned_text, repetition_info = clean_text(baidu_text)

print("âœ… æ•´ç†åçš„æ–‡æœ¬:", cleaned_text)
print("ğŸ” é‡å¤è¯æ£€æµ‹:", repetition_info)
