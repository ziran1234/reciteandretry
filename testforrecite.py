"""
è¯­éŸ³èƒŒè¯µè¯„ä¼°ç³»ç»Ÿ
åŠŸèƒ½ï¼šè¯­éŸ³è¯†åˆ« -> æ–‡æœ¬å¤„ç† -> è¯­éŸ³åˆ†æ -> ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
ä½œè€…ï¼šæ™ºèƒ½åŠ©ç†
ç‰ˆæœ¬ï¼š2.1
"""

import os
import re
import json
import base64
import librosa
import soundfile as sf
import numpy as np
import requests
import torch
import jieba
import psutil
from pathlib import Path
from collections import deque
from sklearn.metrics import accuracy_score
from modelscope import AutoTokenizer, AutoModel

# ################### é…ç½®åŒº ################### #
class Config:
    # ç™¾åº¦è¯­éŸ³è¯†åˆ«
    BAIDU_API_KEY = "yrwh3nyKdMae5766IpfEdbph"
    BAIDU_SECRET_KEY = "OMbwiOvpzpnx2fgrwQioCdJf9Vekb8IC"
    
    # è·¯å¾„é…ç½®
    MODEL_PATH = "/home/humble/addition/paraphrase-multilingual-MiniLM-L12-v2"
    TEMP_DIR = "/tmp/recite_system"
    
    # è¯„ä¼°å‚æ•°
    STUDENT_NAME = "å­¦å‘˜"
    PAUSE_THRESHOLD = 0.3  # åœé¡¿æ£€æµ‹é˜ˆå€¼(ç§’)
    VALID_PAUSE_WINDOW = 0.5  # åˆç†åœé¡¿åŒºé—´(ç§’)
    MAX_SEGMENT_DURATION = 60  # é•¿éŸ³é¢‘åˆ†æ®µæ—¶é•¿(ç§’)
    
    # è¯­ä¹‰å¤„ç†
    DEDUP_THRESHOLD = 0.85  # è¯­ä¹‰å»é‡é˜ˆå€¼

# ################### å·¥å…·å‡½æ•° ################### #
def validate_path(path: str) -> Path:
    """å®‰å…¨è·¯å¾„éªŒè¯"""
    path = Path(path).resolve()
    if not path.exists():
        raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
    return path

def memory_monitor(func):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        process = psutil.Process()
        start_mem = process.memory_info().rss
        result = func(*args, **kwargs)
        delta = (process.memory_info().rss - start_mem) / 1024 / 1024
        if delta > 100:  # 100MBé˜ˆå€¼
            print(f"[å†…å­˜è­¦å‘Š] {func.__name__} å†…å­˜å¢é•¿: {delta:.2f}MB")
        return result
    return wrapper

# ################### æ ¸å¿ƒæ¨¡å— ################### #
class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«æ¨¡å—"""
    
    @staticmethod
    def get_access_token():
        """è·å–ç™¾åº¦APIä»¤ç‰Œ"""
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {
            "grant_type": "client_credentials",
            "client_id": Config.BAIDU_API_KEY,
            "client_secret": Config.BAIDU_SECRET_KEY
        }
        try:
            resp = requests.post(url, params=params, timeout=10)
            return resp.json().get("access_token")
        except Exception as e:
            raise RuntimeError(f"è·å–Tokenå¤±è´¥: {str(e)}")

    @classmethod
    @memory_monitor
    def recognize(cls, audio_path: str) -> str:
        """è¯­éŸ³è½¬æ–‡æœ¬"""
        audio_path = validate_path(audio_path)
        
        # æ£€æŸ¥éŸ³é¢‘æ ¼å¼
        if audio_path.suffix.lower() != ".wav":
            raise ValueError("ä»…æ”¯æŒWAVæ ¼å¼éŸ³é¢‘")
        
        # è·å–è®¿é—®ä»¤ç‰Œ
        token = cls.get_access_token()
        if not token:
            raise RuntimeError("è¯­éŸ³æœåŠ¡ä¸å¯ç”¨")

        # è¯»å–å¹¶ç¼–ç éŸ³é¢‘
        with open(audio_path, "rb") as f:
            audio_data = base64.b64encode(f.read()).decode()

        payload = json.dumps({
            "format": "pcm",
            "rate": 16000,
            "channel": 1,
            "token": token,
            "speech": audio_data,
            "len": len(audio_data)
        })

        try:
            resp = requests.post(
                "https://vop.baidu.com/server_api",
                headers={"Content-Type": "application/json"},
                data=payload,
                timeout=15
            )
            return resp.json().get("result", [""])[0]
        except Exception as e:
            raise RuntimeError(f"è¯†åˆ«å¤±è´¥: {str(e)}")

class TextProcessor:
    """æ–‡æœ¬å¤„ç†æ¨¡å—"""
    
    def __init__(self):
        # åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹
        self.tokenizer, self.model = self._load_model()
        
    def _load_model(self):
        """åŠ è½½è¯­ä¹‰æ¨¡å‹"""
        model_path = validate_path(Config.MODEL_PATH)
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModel.from_pretrained(model_path)
            print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
            return tokenizer, model
        except:
            print("âš ï¸ åŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿æ¨¡å‹")
            return AutoTokenizer.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"), \
                   AutoModel.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    @staticmethod
    def _mean_pooling(model_output, attention_mask):
        """å‡å€¼æ± åŒ–"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    @memory_monitor
    def deduplicate(self, text: str) -> str:
        """è¯­ä¹‰å»é‡"""
        words = jieba.lcut(text)
        if len(words) < 2:
            return text

        cleaned = [words[0]]
        last_emb = None

        for word in words[1:]:
            if word == cleaned[-1]:
                continue

            inputs = self.tokenizer(word, return_tensors="pt", padding=True, truncation=True)
            with torch.no_grad():
                outputs = self.model(**inputs)
            emb = self._mean_pooling(outputs, inputs["attention_mask"]).numpy().flatten()

            if last_emb is not None:
                sim = np.dot(emb, last_emb) / (np.linalg.norm(emb) * np.linalg.norm(last_emb))
                if sim >= Config.DEDUP_THRESHOLD:
                    continue

            cleaned.append(word)
            last_emb = emb

        return "".join(cleaned)

class AudioAnalyzer:
    """è¯­éŸ³åˆ†ææ¨¡å—"""
    
    @staticmethod
    @memory_monitor
    def analyze(audio_path: str, reference_text: str) -> dict:
        """å…¨é¢è¯­éŸ³åˆ†æ"""
        y, sr = librosa.load(str(validate_path(audio_path)), sr=16000)
        duration = librosa.get_duration(y=y, sr=sr)
        
        # è¯­é€Ÿè®¡ç®—
        word_count = len(reference_text.split())
        wpm = word_count / (duration / 60) if duration > 0 else 0
        
        # åœé¡¿åˆ†æ
        valid, invalid = AudioAnalyzer._analyze_pauses(y, sr, reference_text)
        
        return {
            "duration": duration,
            "wpm": wpm,
            "valid_pauses": valid,
            "invalid_pauses": invalid
        }

    @staticmethod
    def _analyze_pauses(y, sr, reference):
        """æ™ºèƒ½åœé¡¿åˆ†æ"""
        intervals = librosa.effects.split(y, top_db=20)
        total_duration = len(y) / sr
        char_duration = total_duration / len(reference) if reference else 0
        
        # ç”Ÿæˆåˆç†åœé¡¿çª—å£
        valid_windows = []
        for idx, char in enumerate(reference):
            if char in "ã€‚ï¼ï¼Ÿï¼›":
                pos = idx * char_duration
                valid_windows.append( (pos - Config.VALID_PAUSE_WINDOW, pos + Config.VALID_PAUSE_WINDOW) )
        
        # åˆ†ç±»åœé¡¿
        valid = invalid = 0
        for start, end in intervals:
            pause_dur = (end - start) / sr
            if pause_dur < Config.PAUSE_THRESHOLD:
                continue
                
            pause_time = start / sr
            if any(lower <= pause_time <= upper for lower, upper in valid_windows):
                valid += 1
            else:
                invalid += 1
                
        return valid, invalid

# ################### ä¸šåŠ¡é€»è¾‘ ################### #
class ReciteEvaluator:
    """èƒŒè¯µè¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self):
        self.text_processor = TextProcessor()
        self.audio_analyzer = AudioAnalyzer()

    def process_audio(self, audio_path: str) -> str:
        """å¤„ç†é•¿éŸ³é¢‘"""
        validate_path(audio_path)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = Path(Config.TEMP_DIR)
        temp_dir.mkdir(exist_ok=True, parents=True)
        
        # é•¿éŸ³é¢‘åˆ†æ®µå¤„ç†
        if librosa.get_duration(filename=audio_path) > Config.MAX_SEGMENT_DURATION:
            return self._process_long_audio(audio_path, temp_dir)
        return SpeechRecognizer.recognize(audio_path)

    def _process_long_audio(self, audio_path: str, temp_dir: Path) -> str:
        """å¤„ç†è¶…è¿‡1åˆ†é’Ÿçš„éŸ³é¢‘"""
        y, sr = librosa.load(audio_path, sr=16000)
        full_text = []
        
        for i, start in enumerate(range(0, len(y), Config.MAX_SEGMENT_DURATION * sr)):
            end = min(start + Config.MAX_SEGMENT_DURATION * sr, len(y))
            seg_path = temp_dir / f"segment_{i}.wav"
            sf.write(seg_path, y[start:end], sr)
            
            try:
                text = SpeechRecognizer.recognize(str(seg_path))
                full_text.append(text)
            finally:
                seg_path.unlink()
                
        return "".join(full_text)

    def generate_report(self, audio_path: str, reference_path: str) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        # è¯»å–æ­£ç¡®æ–‡æœ¬
        with open(validate_path(reference_path), "r", encoding="utf-8") as f:
            reference = f.read().strip()
        
        # è¯­éŸ³è¯†åˆ«
        try:
            raw_text = self.process_audio(audio_path)
            cleaned_text = self.text_processor.deduplicate(raw_text)
        except Exception as e:
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}"

        # è¯­éŸ³åˆ†æ
        audio_stats = self.audio_analyzer.analyze(audio_path, reference)
        
        # ç”ŸæˆæŠ¥å‘Š
        return self._format_report(
            reference=reference,
            hypothesis=cleaned_text,
            audio_stats=audio_stats
        )

    def _format_report(self, reference: str, hypothesis: str, audio_stats: dict) -> str:
        """ç”Ÿæˆæ ¼å¼åŒ–æŠ¥å‘Š"""
        # æ–‡æœ¬å¯¹æ¯”
        cer = self._calculate_cer(reference, hypothesis)
        wer = self._calculate_wer(reference, hypothesis)
        accuracy = (1 - cer) * 100
        
        # æŠ¥å‘Šæ¨¡æ¿
        report = f"""
        ğŸŒŸ {Config.STUDENT_NAME}çš„èƒŒè¯µè¯„ä¼°æŠ¥å‘Š ğŸŒŸ
        {"="*50}
        ğŸ“Š ç»¼åˆè¡¨ç°ï¼š
          å‡†ç¡®ç‡ï¼š{accuracy:.1f}% ({self._accuracy_comment(accuracy)})
          è¯­é€Ÿï¼š{audio_stats['wpm']:.1f} WPM ({self._speed_comment(audio_stats['wpm'])})
          åœé¡¿åˆ†æï¼šåˆç†åœé¡¿ {audio_stats['valid_pauses']} æ¬¡ | å¤šä½™åœé¡¿ {audio_stats['invalid_pauses']} æ¬¡
        
        ğŸ“ è¯¦ç»†åˆ†æï¼š
        {self._segment_analysis(reference, hypothesis)}
        
        ğŸ’¡ å­¦ä¹ å»ºè®®ï¼š
        {self._generate_advice(accuracy, audio_stats['invalid_pauses'])}
        {"="*50}
        """
        return report

    def _segment_analysis(self, ref: str, hyp: str) -> str:
        """åˆ†æ®µåˆ†æ"""
        segments = self._split_text(ref), self._split_text(hyp)
        analysis = []
        
        for i, (r, h) in enumerate(zip(*segments)):
            cer = self._calculate_cer(r, h)
            analysis.append(
                f"ğŸ”¹ æ®µè½{i+1}:\næ­£ç¡®: {r}\nè¯†åˆ«: {h}\nCER: {cer:.2%}\n"
            )
        return "\n".join(analysis)

    @staticmethod
    def _split_text(text: str) -> list:
        """æ™ºèƒ½åˆ†æ®µ"""
        sentences = re.split(r"([ã€‚ï¼ï¼Ÿï¼›])", text)
        segments = []
        current = []
        
        for s in sentences:
            if len("".join(current)) + len(s) > 50:
                segments.append("".join(current))
                current = []
            current.append(s)
            
        if current:
            segments.append("".join(current))
        return segments

    @staticmethod
    def _calculate_cer(ref: str, hyp: str) -> float:
        """å­—ç¬¦é”™è¯¯ç‡"""
        d = np.zeros((len(ref)+1, len(hyp)+1))
        for i in range(len(ref)+1):
            for j in range(len(hyp)+1):
                if i == 0:
                    d[i][j] = j
                elif j == 0:
                    d[i][j] = i
                else:
                    cost = 0 if ref[i-1] == hyp[j-1] else 1
                    d[i][j] = min(d[i-1][j]+1, d[i][j-1]+1, d[i-1][j-1]+cost)
        return d[len(ref)][len(hyp)] / len(ref)

    @staticmethod
    def _calculate_wer(ref: str, hyp: str) -> float:
        """è¯é”™è¯¯ç‡"""
        ref_words = ref.split()
        hyp_words = hyp.split()
        return 1 - accuracy_score(ref_words, hyp_words)

    @staticmethod
    def _accuracy_comment(accuracy: float) -> str:
        """å‡†ç¡®ç‡è¯„è¯­"""
        if accuracy == 100:
            return "å®Œç¾èƒŒè¯µï¼ğŸ‰"
        elif accuracy >= 90:
            return "éå¸¸ä¼˜ç§€ï¼âœ¨"
        elif accuracy >= 75:
            return "è‰¯å¥½ï¼Œç»§ç»­åŠªåŠ›ï¼ğŸ’ª"
        return "éœ€è¦æ›´å¤šç»ƒä¹ ~ğŸ“š"

    @staticmethod
    def _speed_comment(wpm: float) -> str:
        """è¯­é€Ÿè¯„è¯­"""
        avg = 120
        if wpm > avg * 1.2:
            return "è¯­é€Ÿåå¿«"
        if wpm < avg * 0.8:
            return "è¯­é€Ÿåæ…¢"
        return "è¯­é€Ÿé€‚ä¸­"

    @staticmethod
    def _generate_advice(accuracy: float, bad_pauses: int) -> str:
        """ç”Ÿæˆå­¦ä¹ å»ºè®®"""
        advice = []
        if accuracy < 75:
            advice.extend([
                "- é‡ç‚¹ç»ƒä¹ é”™è¯¯æ®µè½",
                "- æ¯å¤©è¿›è¡Œè·Ÿè¯»è®­ç»ƒ"
            ])
        elif bad_pauses > 3:
            advice.extend([
                "- ä½¿ç”¨'å½±å­è·Ÿè¯»æ³•'æ”¹å–„æµç•…åº¦",
                "- å½•éŸ³å›å¬å¯»æ‰¾åœé¡¿é—®é¢˜"
            ])
        else:
            advice.extend([
                "- ä¿æŒè‰¯å¥½å­¦ä¹ èŠ‚å¥",
                "- å°è¯•æƒ…æ„ŸåŒ–è¡¨è¾¾ç»ƒä¹ "
            ])
        return "\n".join(advice)

# ################### ä¸»ç¨‹åº ################### #
if __name__ == "__main__":
    # åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ
    evaluator = ReciteEvaluator()
    
    try:
        # è¾“å…¥è·¯å¾„
        audio_file = "/home/humble/recite/audio.wav"
        reference_file = "/home/humble/recite/correct.txt"
        
        # ç”ŸæˆæŠ¥å‘Š
        report = evaluator.generate_report(audio_file, reference_file)
        print(report)
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè¿è¡Œå‡ºé”™: {str(e)}")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_dir = Path(Config.TEMP_DIR)
        if temp_dir.exists():
            for f in temp_dir.glob("*"):
                f.unlink()
            temp_dir.rmdir()
